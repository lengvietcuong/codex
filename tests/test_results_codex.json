[
    {
        "question": "When using Groq's API to access DeepSeek R1, how can I see the reasoning content?",
        "base_answer": "Set the 'reasoning_format' parameter to either 'parsed' or 'raw' in the API request.",
        "model_answer": "To see the reasoning content when using Groq's API to access DeepSeek R1, you can use the `reasoning_format` parameter in your API request. \n\nThis parameter allows you to control how the model's reasoning process is presented in the response.\n\nHere are the options for the `reasoning_format` parameter:\n\n* `parsed`: Separates reasoning into a dedicated field while keeping the response concise.\n* `raw`: Includes reasoning within think tags in the content.\n* `hidden`: Returns only the final answer for maximum efficiency.\n\nYou can set the `reasoning_format` parameter to one of these options when making a request to the Groq API. \n\nFor example, you can use the following Python code to make a request with the `reasoning_format` parameter set to `raw`:\n\n```python\nimport os\nfrom groq import Groq\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain why fast inference is critical for reasoning models\",\n        }\n    ],\n    model=\"deepseek-r1-distill-llama-70b\",\n    reasoning_format=\"raw\",\n)\n\nprint(chat_completion.choices[0].message.content)\n```\n\nThis will return the response with the reasoning content included in the `content` field.\n\nNote that the `reasoning_format` parameter is only supported for certain models, including DeepSeek R1. \n\nYou can check the documentation for the specific model you are using to see if it supports this parameter.",
        "verdict": "passed"
    },
    {
        "question": "If I want to use Groq with OpenAI's client libraries, what should I set the base URL to?",
        "base_answer": "https://api.groq.com/openai/v1",
        "model_answer": "To use Groq with OpenAI's client libraries, you should set the base URL to `https://api.groq.com/openai/v1`.\\n\\nAdditionally, you need to pass your Groq API key to the `api_key` parameter.\\n\\nHere is an example of how to do this in Python:\\n\\n```python\\nimport os\\nimport openai\\n\\nclient = openai.OpenAI(\\n    base_url=\"https://api.groq.com/openai/v1\",\\n    api_key=os.environ.get(\"GROQ_API_KEY\")\\n)\\n```\\n\\nYou can find your API key [here](/keys).\\n\\nNote that although Groq API is mostly OpenAI compatible, there are a few features that are not supported yet.\\n\\nThese include certain fields in text completions, temperature, and audio transcription and translation.\\n\\nIf you'd like to see support for these features on Groq API, please reach out to us and let us know by submitting a \\\"Feature Request\\\" via \\\"Chat with us\\\" located on the left. We really value your feedback and would love to hear from you!\\n\\nAlso, Groq provides both a Python and JavaScript/Typescript client library.\\n\\nThe Groq Python library provides convenient access to the Groq REST API from any Python 3.7+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients.\\n\\nYou can install the Groq Python library using pip:\\n\\n```shell\\npip install groq\\n```\\n\\nAnd use it like this:\\n\\n```py\\nimport os\\nfrom groq import Groq\\n\\nclient = Groq(\\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\\n)\\n\\nchat_completion = client.chat.completions.create(\\n    messages=[\\n        {\\n            \"role\": \"system\",\\n            \"content\": \"you are a helpful assistant.\"\\n        },\\n        {\\n            \"role\": \"user\",\\n            \"content\": \"Explain the importance of fast language models\",\\n        }\\n    ],\\n    model=\"llama-3.3-70b-versatile\",\\n)\\n\\nprint(chat_completion.choices[0].message.content)\\n```",
        "verdict": "passed"
    },
    {
        "question": "How can I use Groq's API to list all the currently supported models?",
        "correct_answer": "Use the https://api.groq.com/openai/v1/models endpoint.",
        "model_answer": "To list all the currently supported models using Groq's API, you can use the following endpoint:\n\n`https://api.groq.com/openai/v1/models`\n\nYou can send a GET request to this endpoint with your API key in the `Authorization` header.\n\nHere is an example of how you can do this using `curl`:\n\n```bash\ncurl https://api.groq.com/openai/v1/models -s \\\n-H \"Authorization: Bearer $GROQ_API_KEY\"\n```\n\nThis will return a list of all the currently supported models.\n\nPlease note that you need to replace `$GROQ_API_KEY` with your actual API key.\n\nAlso, note that the availability of models may change over time, so it's always a good idea to check the Groq API documentation for the most up-to-date information.\n\nIn addition to the above endpoint, you can also check the Groq API documentation for a list of supported models. The documentation includes information about each model, including its capabilities, limitations, and usage guidelines.\n\nIt's worth noting that the Groq API is designed to be compatible with OpenAI's client libraries, so if you're already using OpenAI's API, you may be able to use the same code with Groq's API with minimal modifications.\n\nIf you have any further questions or need more information, feel free to ask.",
        "verdict": "passed"
    },
    {
        "question": "Does Groq support fine-tuning?",
        "base_answer": "While Groq supports deploying fine-tuned models for enterprise accounts, it does not offer fine-tuning as a service.",
        "model_answer": "Unfortunately, I was unable to find any relevant information on Groq's fine-tuning support. It's possible that the information is not publicly available or is not well-documented. If you have any more questions or need further assistance, feel free to ask!",
        "verdict": "failed"
    },
    {
        "question": "What libraries does Groq have for different programming languages?",
        "base_answer": "Python, JavaScript, C#, Dart/Flutter, PHP, Ruby",
        "model_answer": "Groq has libraries for different programming languages, including Python, JavaScript/Typescript, C#, Dart/Flutter, PHP, and Ruby.\\n\\nThe Groq Python library provides convenient access to the Groq REST API from any Python 3.7+ application, and the JavaScript/Typescript library offers a similar functionality.\\n\\nAdditionally, there are community-built libraries for other languages, such as C#, Dart/Flutter, PHP, and Ruby, which can be used to interact with the Groq API.\\n\\nHere is a list of the libraries mentioned:\\n- Groq Python Library\\n- Groq JavaScript/Typescript Library\\n- jgravelle.GroqAPILibrary (C#)\\n- TAGonSoft.groq-dart (Dart/Flutter)\\n- lucianotonet.groq-php (PHP)\\n- drnic.groq-ruby (Ruby)\\n\\nThese libraries can be used to access the Groq API and utilize its features, such as text completion, speech recognition, and vision processing.",
        "verdict": "passed"
    },
    {
        "question": "What is the pricing for Fireworks AI's latest fine-tuning service?",
        "base_answer": "Free",
        "model_answer": "The pricing for Fireworks AI's latest fine-tuning service is currently free, but it will be charged based on the total number of tokens processed (dataset tokens * number of epochs) in the future. Running inference on fine-tuned models incurs no extra costs outside of base inference fees. For more information, please refer to the Fireworks AI pricing page.",
        "verdict": "passed"
    },
    {
        "question": "Does Fireworks AI support serverless deployment for a LoRA fine-tuned Llama 3.3 70B?",
        "base_answer": "No",
        "model_answer": "Yes, Fireworks AI supports serverless deployment for a LoRA fine-tuned Llama 3.3 70B.\\n\\n### Deploying a LoRA Addon to Serverless\\n\\nFireworks also supports deploying serverless addons for supported base models. To deploy a LoRA addon to serverless, run `firectl deploy` without passing a deployment ID:\\n\\n```bash\\nfirectl deploy <MODEL_ID>\\n```\\n\\nServerless addons are charged by input and output tokens for inference. There is no additional charge for deploying serverless addons.\\n\\nLoRA addons on serverless have higher latency compared with base model inference. This includes LoRA fine-tunes, which are one type of LoRA addon. For faster inference speeds with LoRA addons, we recommend deploying to on-demand.\\n\\nUnused addons may be automatically undeployed after a week.\\n\\n### Deploying to On-Demand\\n\\nAddons may also be deployed in an on-demand deployment of supported base models. To create an on-demand deployment, run:\\n\\n```bash\\nfirectl create deployment \"accounts/fireworks/models/<MODEL_ID of base model>\" --enable-addons\\n```\\n\\nOn-demand deployments are charged by GPU-hour. See [Pricing](https://fireworks.ai/pricing#ondemand) for details.\\n\\nOnce the deployment is ready, deploy the addon to the deployment:\\n\\n```bash\\nfirectl deploy <MODEL_ID> --deployment <DEPLOYMENT_ID>\\n```\\n\\n### Understanding LoRA Performance\\n\\nUnderstand the performance impact of LoRA fine-tuning, optimization strategies, and deployment considerations.\\n\\n#### Why is my LoRA slower than base model inference?\\n\\nThree factors contribute to the observed slowdown:\\n\\n1. **Hardware configuration**: LoRA models on serverless setups often share resources, leading to lower performance compared to dedicated deployments.\\n\\n   Note, this applies for serverless, not on-demand.\\n2. **Unmerged LoRA weights**: Serving LoRA adapters dynamically adds computational overhead during inference. Merging weights removes this overhead.\\n3. **Speculative decoding**: Base models often use speculative decoding to optimize generation speed. Without SD, LoRA fine-tuned models can lag behind.\\n\\n#### How can performance be improved?\\n\\nTo address latency issues and optimize performance:\\n\\n1. **Dedicated deployments**:\\n\\n   - Deploy LoRA models on dedicated hardware to avoid shared resource bottlenecks inherent to serverless setups.\\n2. **Weight merging**:\\n\\n   - Merge LoRA weights into the base model to eliminate runtime overhead.\\n3. **Speculative decoding**:\\n\\n   - Utilize speculative decoding for fine-tuned models with a custom draft model. This can achieve better-than-base performance.\\n\\n#### When should I merge weights vs. serve multiple LoRA adapters?\\n\\n**Scenario** | **Multi-LoRA (Unmerged)** | **Merged LoRA** | **Use case**\\n--- | --- | --- | ---\\nServing multiple fine-tuned variants | Low-latency, single-model deployments | Equivalent to base model\\nHardware needs | Shared or dedicated hardware | Dedicated hardware\\nPerformance impact | Overhead per adapter | Equivalent to base model\\nConcurrency handling | Efficient for experimentation | Limited to one fine-tuned model\\n\\n#### What is the performance impact of weight merging?\\n\\nMerging weights creates a new standalone model indistinguishable from a fully fine-tuned model. Once merged:\\n\\n- Latency matches the base model.\\n- Memory usage is reduced since adapters are no longer dynamically applied.\\n\\n#### What does it take for fine-tuning to match the performance of the base deployment?\\n\\nTo match or exceed base model performance, consider these steps:\\n\\n1. **Speculative decoding**:\\n\\n   - Train a custom draft model optimized for your fine-tuned setup.\\n2. **Dedicated hardware**:\\n\\n   - Avoid serverless deployments to ensure consistent performance.\\n3. **Weight merging**:\\n\\n   - Merge LoRA weights to eliminate inference overhead.\\n\\n### Implementation Guide for Optimizing LoRA Performance\\n\\n#### Steps to Improve Performance\\n\\n1. **Fine-Tune with LoRA**:\\n\\n   - Use LoRA for efficient parameter updates. See our guide on [uploading custom models](https://docs.fireworks.ai/models/uploading-custom-models) for more information.\\n2. **Download and merge weights**:\\n\\n   - Download adapters and merge weights using the PEFT library, [like in this guide](https://docs.fireworks.ai/guides/lora-model-merge).\\n3. **Deploy on dedicated hardware**:\\n\\n   - Deploy merged models for consistent low-latency performance, [like in this guide](https://docs.fireworks.ai/guides/ondemand-deployments).\\n4. **Use speculative decoding**:\\n\\n   - Train and deploy a draft model to further reduce latency.\\n\\n     This is currently an enterprise feature, please reach out for more information.",
        "verdict": "failed"
    },
    {
        "question": "In Fireworks AI, which models can be fine-tuned using LoRA and deployed as serverless?",
        "base_answer": "llama-v3p1-8b-instruct, llama-v3p1-70b-instruct, llama-v3p2-3b-instruct",
        "model_answer": "In Fireworks AI, several models can be fine-tuned using LoRA (Low-Rank Adaptation) and deployed as serverless. Here are the details:\n\n### Supported Base Models for LoRA and Serverless Deployment\n\nThe following base models are supported for low-rank adaptation (LoRA) and can be deployed as LoRA add-ons on Fireworks serverless and on-demand deployments:\n\n1. **accounts/fireworks/models/llama-v3p1-8b-instruct**\n2. **accounts/fireworks/models/llama-v3p1-70b-instruct**\n3. **accounts/fireworks/models/llama-v3p2-3b-instruct**\n\n### Deployment Details\n\n- **Serverless Deployment**: These models can be deployed as LoRA add-ons on Fireworks serverless deployments. Serverless deployment is available for a subset of fine-tuned models. You can check if a model supports serverless deployment by running the command `firectl get <model-id>` or by visiting the [models page](https://fireworks.ai/models).\n- **On-Demand Deployment**: If the base model does not support serverless add-ons, you can use an on-demand deployment to deploy it.\n\n### Fine-Tuning and Deployment Process\n\n1. **Installing firectl**: `firectl` is the command-line utility to manage and deploy resources on the Fireworks AI Platform. You can use it to manage fine-tuning jobs and their resulting models.\n2. **Preparing Your Dataset**: Upload a dataset consisting of a single JSONL file, where each line is a separate training example. The dataset must conform to the schema expected by the OpenAI-compatible Chat Completions API.\n3. **Starting Your Tuning Job**: Use the `firectl create sftj` command to start a fine-tuning job. You can specify the base model, dataset, and output model ID.\n4. **Checking Job Status**: Monitor the progress of the tuning job using `firectl get fine-tuning-job <JOB_ID>`.\n5. **Deploying and Using a Model**: Before using your fine-tuned model for inference, deploy it using the guides on [Deploying a model](/models/deploying#lora-addons) and [Querying text models](/guides/querying-text-models).\n\n### Additional Tuning Options\n\n- **Evaluation**: Specify a separate evaluation dataset if needed.\n- **Early Stopping**: Stop training early if the validation loss does not improve.\n- **Max Context Length**: Increase the max context length if your use case requires context above 8k.\n- **Epochs**: Specify the number of passes over the training data.\n- **Learning Rate**: Control how fast the model updates from data.\n- **LoRA Rank**: Specify the number of parameters to be tuned in your LoRA add-on.\n- **Training Progress and Monitoring**: Integrate with Weights & Biases for observability into the tuning process.\n\n### Example Commands\n\n- **Creating a Dataset**:\n  ```shell\n  firectl create dataset <DATASET_ID> path/to/dataset.jsonl\n  ```\n- **Starting a Fine-Tuning Job**:\n  ```shell\n  firectl create sftj --base-model llama-v3p1-8b-instruct --dataset my_dataset --output-model my_model\n  ```\n- **Checking Job Status**:\n  ```shell\n  firectl get fine-tuning-job <JOB_ID>\n  ```\n- **Deploying a Model**:\n  ```shell\n  firectl deploy model <MODEL_ID>\n  ```\n\nFor more detailed information, you can refer to the [Fine-tuning Models documentation](https://docs.fireworks.ai/fine-tuning/fine-tuning-models).\n\nIf you cannot find the information you are looking for, please let me know how I can further assist you.",
        "verdict": "passed"
    },
    {
        "question": "How to create a fine-tuning job from the terminal in the latest version of Fireworks AI?",
        "base_answer": "firectl create sftj --base-model <MODEL_ID> --dataset <dataset_name> --output-model <desired_model_id>",
        "model_answer": "To create a fine-tuning job from the terminal in the latest version of Fireworks AI, you can use the `firectl` command-line tool.\n\nHere are the steps to follow:\n\n1. **Install `firectl`**:\n   Ensure you have `firectl` installed. You can follow the [Getting Started Guide](https://docs.fireworks.ai/tools-sdks/firectl/firectl) for installation instructions.\n\n2. **Prepare Your Dataset**:\n   Create a dataset in JSONL format where each line is a JSON object containing a `messages` array. Each message should have a `role` (e.g., \"system\", \"user\", \"assistant\") and `content`.\n\n   Example:\n   ```json\n   {\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What color is the sky?\"}, {\"role\": \"assistant\", \"content\": \"blue\"}]}\n   {\"messages\": [{\"role\": \"user\", \"content\": \"What is 1+1?\"}, {\"role\": \"assistant\", \"content\": \"2\"}, {\"role\": \"user\", \"content\": \"Now what is 2+2?\"}, {\"role\": \"assistant\", \"content\": \"4\"}]}\n   ```\n\n   Create the dataset using:\n   ```shell\n   firectl create dataset <DATASET_ID> path/to/dataset.jsonl\n   ```\n\n3. **Start the Fine-Tuning Job**:\n   Use the `firectl create sftj` command to start a structured fine-tuning job. You need to specify the base model, dataset, and output model ID.\n\n   Example:\n   ```shell\n   firectl create sftj --base-model llama-v3p1-8b-instruct --dataset my_dataset --output-model my_model\n   ```\n\n   You can also specify additional tuning options such as learning rate, epochs, and evaluation dataset.\n\n   Example with additional options:\n   ```shell\n   firectl create sftj \\\n   --base-model llama-v3p1-8b-instruct \\\n   --dataset cancerset \\\n   --output-model my-tuned-model \\\n   --job-id my-fine-tuning-job \\\n   --learning-rate 0.0001 \\\n   --epochs 2 \\\n   --early-stop \\\n   --evaluation-dataset my-eval-set\n   ```\n\n4. **Check Job Status**:\n   Monitor the progress of the tuning job using:\n   ```shell\n   firectl get fine-tuning-job <JOB_ID>\n   ```\n\n5. **Deploy and Use the Model**:\n   Once the job is complete, deploy the model for inference. Refer to the guides on [Deploying a model](https://docs.fireworks.ai/models/deploying#lora-addons) and [Querying text models](https://docs.fireworks.ai/guides/querying-text-models) for detailed instructions.\n\n   Example to list models:\n   ```shell\n   firectl list models\n   ```\n\n   Example to get a specific model:\n   ```shell\n   firectl get model <MODEL_ID>\n   ```\n\n6. **Download Model Weights** (Optional):\n   If you need to download the model weights:\n   ```shell\n   firectl download model <model-id> <target local filepath>\n   ```\n\nBy following these steps, you can create and manage a fine-tuning job using the `firectl` tool in Fireworks AI.",
        "verdict": "passed"
    },
    {
        "question": "How do I monitor my training progress when using Firework AI's fine-tuning service?",
        "base_answer": "Connect your fine-tuning job with Weigth & Biases using the flags '--wandb-entity my-org', '--wandb-api-key xxx', and '--wandb-project 'My Project'.",
        "model_answer": "To monitor your training progress when using Firework AI's fine-tuning service, you can use the firectl get fine-tuning-job <JOB_ID> command. This command allows you to check the status of your fine-tuning job and see how it is progressing.\n\nHere is an example of how you can use this command:\n\n1. **Start your fine-tuning job**: First, you need to start a fine-tuning job using the firectl create sftj command. For example:\n\n```shell\nfirectl create sftj --base-model llama-v3p1-8b-instruct --dataset my_dataset --output-model my_model\n```\n\nThis command will return a fine-tuning job ID.\n\n2. **Monitor the job status**: Use the firectl get fine-tuning-job <JOB_ID> command to monitor the progress of your fine-tuning job. For example:\n\n```shell\nfirectl get fine-tuning-job <JOB_ID>\n```\n\nReplace <JOB_ID> with the actual job ID returned from the previous step.\n\n3. **Check the model**: Once the job is complete, you can list your models using:\n\n```shell\nfirectl list models\n```\n\nOr get a specific model using:\n\n```shell\nfirectl get model <MODEL_ID>\n```\n\nAdditionally, you can integrate with Weights & Biases to provide observability into the tuning process. To use this feature, you must have a Weights & Biases account and have provisioned an API key. Here is an example command to set this up:\n\n```shell\nfirectl create sftj \\\n    ...\n    --wandb-entity my-org \\\n    --wandb-api-key xxx \\\n    --wandb-project \"My Project\" \\\n    ...\n```\n\nBy following these steps, you can effectively monitor the progress of your fine-tuning job and ensure that your model is training as expected.",
        "verdict": "passed"
    }
]